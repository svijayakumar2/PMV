{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa490aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not load config: [Errno 2] No such file or directory: 'pmv/configs/config_pure_stackelberg.yaml'\n",
      "Initialized analyzer for runs/pure_stackelberg_experiment/checkpoints\n",
      "Found 0 checkpoints\n",
      "1. Training Progression Analysis\n",
      "No checkpoint found for round 0\n",
      "No checkpoint found for round 1\n",
      "No checkpoint found for round 2\n",
      "No checkpoint found for round 3\n",
      "No checkpoint found for round 4\n",
      "No checkpoint found for round 5\n",
      "No checkpoint found for round 6\n",
      "No checkpoint found for round 7\n",
      "No checkpoint found for round 8\n",
      "No checkpoint found for round 9\n",
      "No training data found in checkpoints\n",
      "\n",
      "2. Reward Distribution Analysis\n",
      "No checkpoint found for round 0\n",
      "No checkpoint found for round 1\n",
      "No checkpoint found for round 2\n",
      "No checkpoint found for round 3\n",
      "No checkpoint found for round 4\n",
      "No checkpoint found for round 5\n",
      "No checkpoint found for round 6\n",
      "No checkpoint found for round 7\n",
      "No checkpoint found for round 8\n",
      "No checkpoint found for round 9\n",
      "\n",
      "3. Sample Solutions from Latest Round\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'empty'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 431\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;66;03m# Usage:\u001b[39;00m\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    430\u001b[0m     \u001b[38;5;66;03m# Run analysis on your actual data\u001b[39;00m\n\u001b[0;32m--> 431\u001b[0m     analyzer, df \u001b[38;5;241m=\u001b[39m \u001b[43mquick_analysis\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mruns/pure_stackelberg_experiment/checkpoints\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpmv/configs/config_pure_stackelberg.yaml\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAnalysis complete! Use these commands for more detailed analysis:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    437\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m- analyzer.display_solution_samples(round_idx)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[4], line 408\u001b[0m, in \u001b[0;36mquick_analysis\u001b[0;34m(checkpoint_dir, config_path)\u001b[0m\n\u001b[1;32m    405\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSneaky rewards: μ=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreward_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msneaky_stats\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, σ=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreward_stats[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msneaky_stats\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstd\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    407\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m3. Sample Solutions from Latest Round\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 408\u001b[0m latest_round \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mround\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    409\u001b[0m analyzer\u001b[38;5;241m.\u001b[39mdisplay_solution_samples(latest_round)\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m analyzer, df\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'empty'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import random\n",
    "from typing import List, Dict, Tuple\n",
    "import seaborn as sns\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import re\n",
    "\n",
    "# Set style for better plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "class ModelAnalyzer:\n",
    "    \"\"\"Analyzer for trained prover-verifier models using real checkpoint data\"\"\"\n",
    "    \n",
    "    def __init__(self, checkpoint_dir: str, config_path: str = None):\n",
    "        self.checkpoint_dir = Path(checkpoint_dir)\n",
    "        self.config = self.load_config(config_path) if config_path else {}\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        \n",
    "        # Cache for loaded data\n",
    "        self.checkpoints_cache = {}\n",
    "        \n",
    "        print(f\"Initialized analyzer for {checkpoint_dir}\")\n",
    "        available_checkpoints = list(self.checkpoint_dir.glob('*.pt'))\n",
    "        print(f\"Found {len(available_checkpoints)} checkpoints\")\n",
    "        for cp in sorted(available_checkpoints):\n",
    "            print(f\"  - {cp.name}\")\n",
    "    \n",
    "    def load_config(self, config_path: str):\n",
    "        \"\"\"Load configuration file\"\"\"\n",
    "        try:\n",
    "            with open(config_path) as f:\n",
    "                return yaml.safe_load(f)\n",
    "        except Exception as e:\n",
    "            print(f\"Could not load config: {e}\")\n",
    "            return {}\n",
    "    \n",
    "    def load_checkpoint(self, round_idx: int):\n",
    "        \"\"\"Load checkpoint for specific round\"\"\"\n",
    "        if round_idx in self.checkpoints_cache:\n",
    "            return self.checkpoints_cache[round_idx]\n",
    "            \n",
    "        # Try different checkpoint naming patterns\n",
    "        patterns = [\n",
    "            f\"kirchner_round_{round_idx:03d}.pt\",\n",
    "            f\"pure_stackelberg_round_{round_idx:03d}.pt\",\n",
    "            f\"round_{round_idx:03d}.pt\"\n",
    "        ]\n",
    "        \n",
    "        checkpoint_file = None\n",
    "        for pattern in patterns:\n",
    "            potential_file = self.checkpoint_dir / pattern\n",
    "            if potential_file.exists():\n",
    "                checkpoint_file = potential_file\n",
    "                break\n",
    "        \n",
    "        if checkpoint_file is None:\n",
    "            print(f\"No checkpoint found for round {round_idx}\")\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            checkpoint = torch.load(checkpoint_file, map_location='cpu')\n",
    "            self.checkpoints_cache[round_idx] = checkpoint\n",
    "            print(f\"Loaded checkpoint from round {round_idx}: {checkpoint_file.name}\")\n",
    "            return checkpoint\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading checkpoint for round {round_idx}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def extract_replay_buffer_stats(self, replay_buffer: List[Tuple]) -> Dict:\n",
    "        \"\"\"Extract statistics from replay buffer\"\"\"\n",
    "        if not replay_buffer:\n",
    "            return {}\n",
    "        \n",
    "        helpful_items = [item for item in replay_buffer if len(item) > 5 and item[5] == 'helpful']\n",
    "        sneaky_items = [item for item in replay_buffer if len(item) > 5 and item[5] == 'sneaky']\n",
    "        \n",
    "        stats = {\n",
    "            'total_items': len(replay_buffer),\n",
    "            'helpful_count': len(helpful_items),\n",
    "            'sneaky_count': len(sneaky_items),\n",
    "        }\n",
    "        \n",
    "        # Extract rewards if available\n",
    "        if helpful_items and len(helpful_items[0]) > 3:\n",
    "            helpful_rewards = [item[3] for item in helpful_items if isinstance(item[3], (int, float))]\n",
    "            if helpful_rewards:\n",
    "                stats['helpful_reward_mean'] = np.mean(helpful_rewards)\n",
    "                stats['helpful_reward_std'] = np.std(helpful_rewards)\n",
    "        \n",
    "        if sneaky_items and len(sneaky_items[0]) > 3:\n",
    "            sneaky_rewards = [item[3] for item in sneaky_items if isinstance(item[3], (int, float))]\n",
    "            if sneaky_rewards:\n",
    "                stats['sneaky_reward_mean'] = np.mean(sneaky_rewards)\n",
    "                stats['sneaky_reward_std'] = np.std(sneaky_rewards)\n",
    "        \n",
    "        # Extract solution lengths\n",
    "        if helpful_items and len(helpful_items[0]) > 2:\n",
    "            helpful_solutions = [item[2] for item in helpful_items if isinstance(item[2], str)]\n",
    "            if helpful_solutions:\n",
    "                stats['helpful_solution_length_mean'] = np.mean([len(sol.split()) for sol in helpful_solutions])\n",
    "        \n",
    "        if sneaky_items and len(sneaky_items[0]) > 2:\n",
    "            sneaky_solutions = [item[2] for item in sneaky_items if isinstance(item[2], str)]\n",
    "            if sneaky_solutions:\n",
    "                stats['sneaky_solution_length_mean'] = np.mean([len(sol.split()) for sol in sneaky_solutions])\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def analyze_training_progression(self, max_rounds: int = None) -> pd.DataFrame:\n",
    "        \"\"\"Analyze actual training progression from checkpoints\"\"\"\n",
    "        if max_rounds is None:\n",
    "            # Auto-detect max rounds\n",
    "            checkpoint_files = list(self.checkpoint_dir.glob('*.pt'))\n",
    "            round_numbers = []\n",
    "            for f in checkpoint_files:\n",
    "                match = re.search(r'round_(\\d+)', f.name)\n",
    "                if match:\n",
    "                    round_numbers.append(int(match.group(1)))\n",
    "            max_rounds = max(round_numbers) + 1 if round_numbers else 10\n",
    "        \n",
    "        progression_data = []\n",
    "        \n",
    "        for round_idx in range(max_rounds):\n",
    "            checkpoint = self.load_checkpoint(round_idx)\n",
    "            if checkpoint is None:\n",
    "                continue\n",
    "            \n",
    "            round_data = {'round': round_idx}\n",
    "            \n",
    "            # Extract replay buffer statistics\n",
    "            if 'replay_buffer' in checkpoint:\n",
    "                replay_stats = self.extract_replay_buffer_stats(checkpoint['replay_buffer'])\n",
    "                round_data.update(replay_stats)\n",
    "            \n",
    "            # Extract any other metrics from checkpoint\n",
    "            for key in ['round', 'pure_stackelberg', 'config']:\n",
    "                if key in checkpoint:\n",
    "                    if key == 'config' and isinstance(checkpoint[key], dict):\n",
    "                        # Extract relevant config info\n",
    "                        if 'training' in checkpoint[key]:\n",
    "                            round_data.update({f\"config_{k}\": v for k, v in checkpoint[key]['training'].items() \n",
    "                                             if isinstance(v, (int, float, str))})\n",
    "                    else:\n",
    "                        round_data[key] = checkpoint[key]\n",
    "            \n",
    "            progression_data.append(round_data)\n",
    "        \n",
    "        return pd.DataFrame(progression_data)\n",
    "    \n",
    "    def plot_kirchner_style_training(self, max_rounds: int = None):\n",
    "        \"\"\"Plot training dynamics in Kirchner paper style\"\"\"\n",
    "        df = self.analyze_training_progression(max_rounds)\n",
    "        \n",
    "        if df.empty:\n",
    "            print(\"No training data found in checkpoints\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"Loaded data for {len(df)} rounds\")\n",
    "        print(\"Available columns:\", df.columns.tolist())\n",
    "        \n",
    "        # Create the plot\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "        \n",
    "        # Plot 1: Episode counts over rounds\n",
    "        if 'helpful_count' in df.columns and 'sneaky_count' in df.columns:\n",
    "            axes[0,0].plot(df['round'], df['helpful_count'], 'o-', label='Helpful', color='green', linewidth=2)\n",
    "            axes[0,0].plot(df['round'], df['sneaky_count'], 'o-', label='Sneaky', color='red', linewidth=2)\n",
    "            axes[0,0].set_xlabel('Round')\n",
    "            axes[0,0].set_ylabel('Episode Count')\n",
    "            axes[0,0].set_title('Episodes Generated per Round')\n",
    "            axes[0,0].legend()\n",
    "            axes[0,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 2: Reward evolution\n",
    "        if 'helpful_reward_mean' in df.columns and 'sneaky_reward_mean' in df.columns:\n",
    "            axes[0,1].plot(df['round'], df['helpful_reward_mean'], 'o-', label='Helpful', color='green', linewidth=2)\n",
    "            axes[0,1].plot(df['round'], df['sneaky_reward_mean'], 'o-', label='Sneaky', color='red', linewidth=2)\n",
    "            \n",
    "            # Add error bars if std available\n",
    "            if 'helpful_reward_std' in df.columns:\n",
    "                axes[0,1].fill_between(df['round'], \n",
    "                                     df['helpful_reward_mean'] - df['helpful_reward_std'],\n",
    "                                     df['helpful_reward_mean'] + df['helpful_reward_std'],\n",
    "                                     alpha=0.2, color='green')\n",
    "            if 'sneaky_reward_std' in df.columns:\n",
    "                axes[0,1].fill_between(df['round'], \n",
    "                                     df['sneaky_reward_mean'] - df['sneaky_reward_std'],\n",
    "                                     df['sneaky_reward_mean'] + df['sneaky_reward_std'],\n",
    "                                     alpha=0.2, color='red')\n",
    "            \n",
    "            axes[0,1].set_xlabel('Round')\n",
    "            axes[0,1].set_ylabel('Average Reward')\n",
    "            axes[0,1].set_title('Reward Evolution Over Training')\n",
    "            axes[0,1].legend()\n",
    "            axes[0,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 3: Total data accumulation\n",
    "        if 'total_items' in df.columns:\n",
    "            cumulative_data = df['total_items'].fillna(0)\n",
    "            axes[1,0].plot(df['round'], cumulative_data, 'o-', color='blue', linewidth=2)\n",
    "            axes[1,0].set_xlabel('Round')\n",
    "            axes[1,0].set_ylabel('Total Items in Replay Buffer')\n",
    "            axes[1,0].set_title('Data Accumulation')\n",
    "            axes[1,0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot 4: Solution length evolution\n",
    "        if 'helpful_solution_length_mean' in df.columns and 'sneaky_solution_length_mean' in df.columns:\n",
    "            axes[1,1].plot(df['round'], df['helpful_solution_length_mean'], 'o-', \n",
    "                          label='Helpful', color='green', linewidth=2)\n",
    "            axes[1,1].plot(df['round'], df['sneaky_solution_length_mean'], 'o-', \n",
    "                          label='Sneaky', color='red', linewidth=2)\n",
    "            axes[1,1].set_xlabel('Round')\n",
    "            axes[1,1].set_ylabel('Average Solution Length (words)')\n",
    "            axes[1,1].set_title('Solution Complexity Evolution')\n",
    "            axes[1,1].legend()\n",
    "            axes[1,1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def sample_real_solutions(self, round_idx: int, num_samples: int = 3) -> Dict:\n",
    "        \"\"\"Sample actual solutions from checkpoint data\"\"\"\n",
    "        checkpoint = self.load_checkpoint(round_idx)\n",
    "        if not checkpoint or 'replay_buffer' not in checkpoint:\n",
    "            print(f\"No replay buffer data for round {round_idx}\")\n",
    "            return {}\n",
    "        \n",
    "        replay_buffer = checkpoint['replay_buffer']\n",
    "        helpful_items = [item for item in replay_buffer if len(item) > 5 and item[5] == 'helpful']\n",
    "        sneaky_items = [item for item in replay_buffer if len(item) > 5 and item[5] == 'sneaky']\n",
    "        \n",
    "        samples = {\n",
    "            'helpful': random.sample(helpful_items, min(num_samples, len(helpful_items))),\n",
    "            'sneaky': random.sample(sneaky_items, min(num_samples, len(sneaky_items)))\n",
    "        }\n",
    "        \n",
    "        return samples\n",
    "    \n",
    "    def display_solution_samples(self, round_idx: int, num_samples: int = 2):\n",
    "        \"\"\"Display actual solution samples from training\"\"\"\n",
    "        samples = self.sample_real_solutions(round_idx, num_samples)\n",
    "        \n",
    "        if not samples:\n",
    "            print(f\"No samples available for round {round_idx}\")\n",
    "            return\n",
    "        \n",
    "        print(f\"=== SOLUTION SAMPLES FROM ROUND {round_idx} ===\\n\")\n",
    "        \n",
    "        for role in ['helpful', 'sneaky']:\n",
    "            if role in samples and samples[role]:\n",
    "                print(f\"{role.upper()} SOLUTIONS:\")\n",
    "                print(\"=\" * 50)\n",
    "                \n",
    "                for i, item in enumerate(samples[role]):\n",
    "                    if len(item) > 2 and isinstance(item[2], str):\n",
    "                        solution = item[2]\n",
    "                        reward = item[3] if len(item) > 3 else \"N/A\"\n",
    "                        \n",
    "                        print(f\"\\nSample {i+1} (Reward: {reward}):\")\n",
    "                        print(\"-\" * 30)\n",
    "                        print(solution[:500] + (\"...\" if len(solution) > 500 else \"\"))\n",
    "                        print()\n",
    "                \n",
    "                print(\"=\" * 50)\n",
    "                print()\n",
    "    \n",
    "    def compare_rounds_detailed(self, round1: int, round2: int):\n",
    "        \"\"\"Detailed comparison between two rounds\"\"\"\n",
    "        df = self.analyze_training_progression()\n",
    "        \n",
    "        if round1 not in df['round'].values or round2 not in df['round'].values:\n",
    "            print(f\"Data not available for rounds {round1} or {round2}\")\n",
    "            return\n",
    "        \n",
    "        data1 = df[df['round'] == round1].iloc[0]\n",
    "        data2 = df[df['round'] == round2].iloc[0]\n",
    "        \n",
    "        print(f\"\\n=== DETAILED COMPARISON: ROUND {round1} vs ROUND {round2} ===\")\n",
    "        \n",
    "        # Compare numeric columns\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        for col in numeric_cols:\n",
    "            if col in data1 and col in data2 and col != 'round':\n",
    "                val1 = data1[col]\n",
    "                val2 = data2[col]\n",
    "                change = val2 - val1\n",
    "                pct_change = (change / val1 * 100) if val1 != 0 else 0\n",
    "                \n",
    "                print(f\"{col}:\")\n",
    "                print(f\"  Round {round1}: {val1:.3f}\")\n",
    "                print(f\"  Round {round2}: {val2:.3f}\")\n",
    "                print(f\"  Change: {change:+.3f} ({pct_change:+.1f}%)\")\n",
    "                print()\n",
    "        \n",
    "        return data1, data2\n",
    "    \n",
    "    def analyze_reward_distribution(self, max_rounds: int = None):\n",
    "        \"\"\"Analyze reward distributions across training\"\"\"\n",
    "        df = self.analyze_training_progression(max_rounds)\n",
    "        \n",
    "        if df.empty:\n",
    "            return\n",
    "        \n",
    "        # Collect all rewards from all rounds\n",
    "        all_helpful_rewards = []\n",
    "        all_sneaky_rewards = []\n",
    "        round_labels_helpful = []\n",
    "        round_labels_sneaky = []\n",
    "        \n",
    "        for round_idx in df['round']:\n",
    "            checkpoint = self.load_checkpoint(round_idx)\n",
    "            if checkpoint and 'replay_buffer' in checkpoint:\n",
    "                helpful_items = [item for item in checkpoint['replay_buffer'] \n",
    "                               if len(item) > 5 and item[5] == 'helpful']\n",
    "                sneaky_items = [item for item in checkpoint['replay_buffer'] \n",
    "                              if len(item) > 5 and item[5] == 'sneaky']\n",
    "                \n",
    "                helpful_rewards = [item[3] for item in helpful_items \n",
    "                                 if len(item) > 3 and isinstance(item[3], (int, float))]\n",
    "                sneaky_rewards = [item[3] for item in sneaky_items \n",
    "                                if len(item) > 3 and isinstance(item[3], (int, float))]\n",
    "                \n",
    "                all_helpful_rewards.extend(helpful_rewards)\n",
    "                all_sneaky_rewards.extend(sneaky_rewards)\n",
    "                round_labels_helpful.extend([round_idx] * len(helpful_rewards))\n",
    "                round_labels_sneaky.extend([round_idx] * len(sneaky_rewards))\n",
    "        \n",
    "        # Plot distributions\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Overall distribution\n",
    "        axes[0].hist(all_helpful_rewards, alpha=0.6, label='Helpful', bins=30, color='green')\n",
    "        axes[0].hist(all_sneaky_rewards, alpha=0.6, label='Sneaky', bins=30, color='red')\n",
    "        axes[0].set_xlabel('Reward')\n",
    "        axes[0].set_ylabel('Frequency')\n",
    "        axes[0].set_title('Overall Reward Distribution')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Reward evolution over rounds\n",
    "        if len(set(round_labels_helpful)) > 1:\n",
    "            reward_by_round = {}\n",
    "            for round_idx in sorted(set(round_labels_helpful)):\n",
    "                helpful_round_rewards = [r for r, rl in zip(all_helpful_rewards, round_labels_helpful) if rl == round_idx]\n",
    "                sneaky_round_rewards = [r for r, rl in zip(all_sneaky_rewards, round_labels_sneaky) if rl == round_idx]\n",
    "                \n",
    "                if helpful_round_rewards:\n",
    "                    reward_by_round[f\"helpful_{round_idx}\"] = helpful_round_rewards\n",
    "                if sneaky_round_rewards:\n",
    "                    reward_by_round[f\"sneaky_{round_idx}\"] = sneaky_round_rewards\n",
    "            \n",
    "            # Box plot by round\n",
    "            rounds = sorted(set(round_labels_helpful))\n",
    "            helpful_means = [np.mean([r for r, rl in zip(all_helpful_rewards, round_labels_helpful) if rl == round_idx]) \n",
    "                           for round_idx in rounds]\n",
    "            sneaky_means = [np.mean([r for r, rl in zip(all_sneaky_rewards, round_labels_sneaky) if rl == round_idx]) \n",
    "                          for round_idx in rounds]\n",
    "            \n",
    "            axes[1].plot(rounds, helpful_means, 'o-', label='Helpful', color='green', linewidth=2)\n",
    "            axes[1].plot(rounds, sneaky_means, 'o-', label='Sneaky', color='red', linewidth=2)\n",
    "            axes[1].set_xlabel('Round')\n",
    "            axes[1].set_ylabel('Mean Reward')\n",
    "            axes[1].set_title('Reward Evolution by Round')\n",
    "            axes[1].legend()\n",
    "            axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'helpful_rewards': all_helpful_rewards,\n",
    "            'sneaky_rewards': all_sneaky_rewards,\n",
    "            'helpful_stats': {\n",
    "                'mean': np.mean(all_helpful_rewards),\n",
    "                'std': np.std(all_helpful_rewards),\n",
    "                'count': len(all_helpful_rewards)\n",
    "            },\n",
    "            'sneaky_stats': {\n",
    "                'mean': np.mean(all_sneaky_rewards),\n",
    "                'std': np.std(all_sneaky_rewards),\n",
    "                'count': len(all_sneaky_rewards)\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Quick analysis functions\n",
    "def quick_analysis(checkpoint_dir: str, config_path: str = None):\n",
    "    \"\"\"Run quick analysis of training results\"\"\"\n",
    "    analyzer = ModelAnalyzer(checkpoint_dir, config_path)\n",
    "    \n",
    "    print(\"1. Training Progression Analysis\")\n",
    "    df = analyzer.plot_kirchner_style_training()\n",
    "    \n",
    "    print(\"\\n2. Reward Distribution Analysis\")\n",
    "    reward_stats = analyzer.analyze_reward_distribution()\n",
    "    if reward_stats:\n",
    "        print(f\"Helpful rewards: μ={reward_stats['helpful_stats']['mean']:.3f}, σ={reward_stats['helpful_stats']['std']:.3f}\")\n",
    "        print(f\"Sneaky rewards: μ={reward_stats['sneaky_stats']['mean']:.3f}, σ={reward_stats['sneaky_stats']['std']:.3f}\")\n",
    "    \n",
    "    print(\"\\n3. Sample Solutions from Latest Round\")\n",
    "    latest_round = df['round'].max() if not df.empty else 0\n",
    "    analyzer.display_solution_samples(latest_round)\n",
    "    \n",
    "    return analyzer, df\n",
    "\n",
    "def compare_early_vs_late(analyzer, early_round: int = 0, late_round: int = None):\n",
    "    \"\"\"Compare early vs late training rounds\"\"\"\n",
    "    if late_round is None:\n",
    "        df = analyzer.analyze_training_progression()\n",
    "        late_round = df['round'].max() if not df.empty else 5\n",
    "    \n",
    "    print(f\"Comparing Round {early_round} vs Round {late_round}\")\n",
    "    analyzer.compare_rounds_detailed(early_round, late_round)\n",
    "    \n",
    "    print(f\"\\nSolution samples from Round {early_round}:\")\n",
    "    analyzer.display_solution_samples(early_round, 1)\n",
    "    \n",
    "    print(f\"\\nSolution samples from Round {late_round}:\")\n",
    "    analyzer.display_solution_samples(late_round, 1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    analyzer, df = quick_analysis(\n",
    "        checkpoint_dir=\"runs/pure_stackelberg_experiment/checkpoints\",\n",
    "        config_path=\"pmv/configs/config_pure_stackelberg.yaml\"\n",
    "    )\n",
    "    \n",
    "    print(\"\\nother commands:\")\n",
    "    print(\"- analyzer.display_solution_samples(round_idx)\")\n",
    "    print(\"- analyzer.compare_rounds_detailed(round1, round2)\")\n",
    "    print(\"- compare_early_vs_late(analyzer)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0acc508",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/saranya/Desktop/PMV/pmv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c140e3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
