# PMV Game - Default Configuration
# Matches paper Section 6 and Algorithm 1

model:
  prover_model: "Qwen/Qwen2-0.5B"
  verifier_model: "Qwen/Qwen2-0.5B"
  num_verifiers: 3
  use_quantization: true
  prover_lora_r: 8
  prover_lora_alpha: 16
  verifier_lora_r: 8
  verifier_lora_alpha: 16
  aggregator_hidden_dim: 64

training:
  rounds: 10
  mu_0: 0.5                    # Prior P(τ=H), fixed at 1/2 during training (Section 5)
  bootstrap_episodes: 50
  collect_episodes: 50
  max_buffer_size: 5000        # Replay buffer capacity (Section 6.4)
  debate_rounds: 2             # Irving-style debate rounds

  # Phase 1: Nash update (Algorithm 3)
  oversight_rule: "supervised"  # Options: supervised, pe_min, pe_margin, softmin, average, median
  verifier_lr: 1.0e-4
  verifier_epochs: 3
  verifier_batch_size: 8
  verifier_dropout: 0.0        # Dropout p for SoftMin mask (Appendix D)
  softmin_beta: 5.0            # Temperature β for SoftMin utility

  # Phase 2: Stackelberg PPO (Algorithm 2)
  prover_lr: 1.0e-5
  ppo_epochs: 4
  clip_ratio: 0.2
  kl_coeff: 0.1

  # Reward function (Equation 4)
  alpha_1: 2.0
  beta_1: 1.0
  alpha_2: 0.0
  beta_2: -1.0
